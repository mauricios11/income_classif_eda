{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (03) modelado para clasificación de ingresos: Random Forest\n",
    "\n",
    "en este punto del proyecto, se ha hecho:\n",
    "* Un EDA a fondo en el dataset de clasificación de ingresos *(notebook 00)*\n",
    "* Se han imputado columnas  con `NaN` *(notebook 01)*\n",
    "* Se han analizado realciones no lineales *(notebook 02)*\n",
    "* Se ha balanceado la columna target `income` *(notebook 02)*\n",
    "* Se han analizado los outliers *(notebook 02)*\n",
    "\n",
    "ahora, comenzaremos el modelado de un `Random Forest`\n",
    "\n",
    "**objetivos**:\n",
    "* **(1)** hacer un modelado inicial comenzando con dos dataset: uno con outliers y otro sin outliers\n",
    "    * evalueremos el desempeño de ambos modelos, teniendo encuenta métricas tales como: *F1-score, recall, accuracy* aplicaremos un cross validation\n",
    "    * evaluar el  posible **overfitting** y disminuirlo\n",
    "    \n",
    "* **(2)** entrenamiento final del modelo con el dataset con mejor desempeño \n",
    "\n",
    "entorno: eda (anaconda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## importación de librerías, módulos propios y datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy   as np\n",
    "import pandas  as pd\n",
    "import pprint\n",
    "\n",
    "from sklearn.compose          import   ColumnTransformer\n",
    "from sklearn.ensemble         import   RandomForestClassifier\n",
    "from sklearn.model_selection  import  (train_test_split as tts,\n",
    "                                       cross_val_score as cvs,\n",
    "                                       GridSearchCV)\n",
    "from sklearn.metrics          import   classification_report, f1_score\n",
    "from sklearn.pipeline         import   Pipeline\n",
    "from sklearn.preprocessing    import  (OneHotEncoder as OHE,\n",
    "                                       StandardScaler as SS,\n",
    "                                       FunctionTransformer)\n",
    "from typing import List, Dict\n",
    "\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# own modules\n",
    "from import_modules import import_to_nb\n",
    "\n",
    "# plotting & eda functions\n",
    "import_to_nb(directory= 'scripts', show_content= False)\n",
    "\n",
    "# lists, dicts & auxiliar functions (for this notebook)\n",
    "import_to_nb(directory= 'modules', show_content= False) \n",
    "\n",
    "#-# directory: scripts\n",
    "from load_data import Loader\n",
    "from utils     import Utils\n",
    "from utils_initial_exploration     import InitialExploration\n",
    "from utils_categorical_plots       import CategoricalPlots\n",
    "from utils_classif_models_plots    import ClassifModelsPlots\n",
    "from utils_numerical_plots         import NumericalPlots\n",
    "\n",
    "#-# directory: modules\n",
    "from module_modeling               import ModelingMethods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load          = Loader()\n",
    "utils         = Utils()\n",
    "initial_exp   = InitialExploration()\n",
    "cat_plots     = CategoricalPlots()\n",
    "classif_plots = ClassifModelsPlots()\n",
    "num_plots     = NumericalPlots()\n",
    "model_methods = ModelingMethods()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore',  category= FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load appereance\n",
    "utils.load_appereance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_outliers    = load.load_data(file_name= 'adult_with_outliers'   , dir= 'clean')\n",
    "df_without_outliers = load.load_data(file_name= 'adult_without_outliers', dir= 'clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'hours_per_week', 'marital_status', 'relationship', 'education',\n",
       "       'occupation', 'capital_net', 'income'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_outliers.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF shapes:\n",
      "with outliers:    (74256, 8)\n",
      "without outliers: (51832, 8)\n"
     ]
    }
   ],
   "source": [
    "text_000 = f'DF shapes:\\nwith outliers:    {df_with_outliers.shape}\\nwithout outliers: {df_without_outliers.shape}'\n",
    "print(text_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comienzo del análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'hours_per_week', 'marital_status', 'relationship', 'education',\n",
       "       'occupation', 'capital_net', 'income'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_outliers.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*ganancia neta = capital_gain - capital_loss*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3.1) elección de dataset para entrenamiento final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluando dataset: with_outliers\n",
      "F1-score en test: 0.8547\n",
      "F1-score (CV): 0.8469\n",
      "                      0             1  accuracy     macro avg  weighted avg\n",
      "precision      0.864807      0.840479  0.852224      0.852643      0.852644\n",
      "recall         0.834994      0.869456  0.852224      0.852225      0.852224\n",
      "f1-score       0.849639      0.854722  0.852224      0.852181      0.852180\n",
      "support    11139.000000  11138.000000  0.852224  22277.000000  22277.000000\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*- \n",
      "\n",
      "\n",
      "Evaluando dataset: without_outliers\n",
      "F1-score en test: 0.8091\n",
      "F1-score (CV): 0.8148\n",
      "                     0            1  accuracy     macro avg  weighted avg\n",
      "precision     0.848175     0.791857  0.821222      0.820016      0.822374\n",
      "recall        0.816164     0.827204  0.821222      0.821684      0.821222\n",
      "f1-score      0.831862     0.809145  0.821222      0.820503      0.821454\n",
      "support    8426.000000  7124.000000  0.821222  15550.000000  15550.000000\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluación de df's con y sin outliers (cv, f1_score, classification_report)\n",
    "# path original: ./modules/module_modeling.py\n",
    "model_methods.evaluate_datasets(df_with_outliers, df_without_outliers, target_col= 'income')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b style=\"font-size: 1.5em;\">🔍 revisión de resultados obtenidos</i></b>\n",
    "    <p>Los resultados son muy similares (<b>~0.854</b> vs <b>~0.809</b> en entrenamiento, con diferencias similares en test)\n",
    "    <p>El dataframe <b>con outliers</b> tiene un desempeño ligeramente mejor que el que no tiene tanto en el conjunto de entrenamiento como el de prueba. En general, esto ocurre en todas las métricas obtenidas.</p>\n",
    "    <p>Esta situación sugiere lo siguiente:</p>\n",
    "    <ul>\n",
    "        <li>los outliers contienen información relevante para la generalización</li>\n",
    "        <li>No es estricamente necesario eliminar los outliers para este modelo, en este contexto de datos.</li>\n",
    "        <li><b>support</b> es mucho mayor con outliers que con sin outliers, lo cuál tiene sentido porque el dataset en cuestión tiene más ocurrencias. Esto juega un factor a favor, dado que se tienen más datos para llegar a una mejor generalización.</li>\n",
    "    </ul>\n",
    "    <p>Un factor a favor es que se usará un Random Forest <i>(poco sensible a los outliers)</i>.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'hours_per_week', 'marital_status', 'relationship', 'education',\n",
       "       'occupation', 'capital_net', 'income'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_outliers.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3.2) Estrategia de modelado final\n",
    "\n",
    "Columnas seleccionadas para el entrenamiento *(resultado de feature importances + mutual information)*:\n",
    "* `'capital_net', 'age', 'hours_per_week', 'marital_status', 'relationship','education', 'occupation', 'income'`\n",
    "\n",
    "se usará un **GridSearch** para encontrar los mejores parámetros para usar en el modelo, haciendo incapié en:\n",
    "* número de árboles (`n_estimators`)\n",
    "* profundidad (`max_depth`)\n",
    "* mínimo de muestras por hoja (`min_samples_leaf`)\n",
    "* se evaluará el desempeño de cada iteración con los ya conocidos: *f1-score, recall, accuracy*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../models/randomforest_probe_.pkl'\n",
    "params_grid = {'n_estimators'      : [50, 250, 300],      # número de árboles\n",
    "                'max_depth'        : [None, 10, 20, 30],  # qué tan profundo será del árbol\n",
    "                'min_samples_split': [2, 5, 10, 20],      # mínimo (dividir nodos)\n",
    "                'min_samples_leaf' : [1, 2, 4, 5]}        # mínimo (muestras por hoja)\n",
    "\n",
    "##path: ./modules/module_modeling.py (func 4)\n",
    "# results_pipeline = (model_methods\n",
    "#                     .training_rf_bagging(df= df_without_outliers,\n",
    "#                                             target_col= 'income',\n",
    "#                                             param_grid= params_grid,\n",
    "#                                             output_model_path= model_path)\n",
    "#                   )\n",
    "\n",
    "#---- RESULTADOS: ----\n",
    "#Mejores hiperparámetros: {'classifier__max_depth': 20, 'classifier__min_samples_leaf': 1,\n",
    "#                          'classifier__min_samples_split': 2, 'classifier__n_estimators': 300}\n",
    "# F1-score en test: 0.8347\n",
    "# Classification report:\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#           no       0.88      0.80      0.84      8426\n",
    "#          yes       0.79      0.87      0.83      7124\n",
    "\n",
    "#     accuracy                           0.83     15550\n",
    "#    macro avg       0.84      0.84      0.83     15550\n",
    "# weighted avg       0.84      0.83      0.84     15550"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "los resultados podrían mejorarse, implementaremos una nueva estrategia con stacking de:\n",
    "* RandomForestClassifier + XGBoostClassifier *(optimizando cada uno por grid search)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizando Random Forest...\n",
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   6.7s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   6.7s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   6.7s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=150; total time=  10.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=150; total time=  10.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=150; total time=  10.3s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   6.7s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   6.7s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   6.8s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=300; total time=  20.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=300; total time=  20.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=150; total time=  10.0s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=150; total time=  10.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=150; total time=  10.4s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   6.6s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   6.6s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   6.7s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=300; total time=  20.5s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=300; total time=  19.9s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=150; total time=   9.9s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=150; total time=   9.9s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=150; total time=  10.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=300; total time=  20.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=300; total time=  20.3s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   6.8s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   6.7s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   6.7s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300; total time=  19.8s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300; total time=  19.8s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=150; total time=  10.0s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=150; total time=  10.0s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=150; total time=  10.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300; total time=  20.1s\n",
      "[CV] END classifier__max_depth=15, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=100; total time=  14.3s\n",
      "[CV] END classifier__max_depth=15, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=100; total time=  14.5s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=300; total time=  19.9s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=300; total time=  19.8s\n",
      "[CV] END classifier__max_depth=15, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=100; total time=  14.4s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=300; total time=  19.9s\n",
      "[CV] END classifier__max_depth=15, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=150; total time=  21.7s\n",
      "[CV] END classifier__max_depth=15, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=150; total time=  21.5s\n",
      "[CV] END classifier__max_depth=15, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=100; total time=  14.0s\n",
      "[CV] END classifier__max_depth=15, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=100; total time=  13.9s\n",
      "[CV] END classifier__max_depth=15, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=150; total time=  22.1s\n",
      "[CV] END classifier__max_depth=15, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=100; total time=  13.9s\n",
      "[CV] END classifier__max_depth=15, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=150; total time=  20.4s\n",
      "[CV] END classifier__max_depth=15, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=150; total time=  19.9s\n",
      "[CV] END classifier__max_depth=15, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=150; total time=  20.0s\n",
      "[CV] END classifier__max_depth=15, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=300; total time=  42.8s\n",
      "[CV] END classifier__max_depth=15, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=300; total time=  42.3s\n",
      "[CV] END classifier__max_depth=15, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=300; total time=  43.3s\n",
      "[CV] END classifier__max_depth=15, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100; total time=  12.2s\n",
      "[CV] END classifier__max_depth=15, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100; total time=  12.3s\n",
      "[CV] END classifier__max_depth=15, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100; total time=  12.4s\n",
      "[CV] END classifier__max_depth=15, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=300; total time=  39.2s\n",
      "[CV] END classifier__max_depth=15, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=150; total time=  18.3s\n",
      "[CV] END classifier__max_depth=15, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=150; total time=  18.1s\n",
      "[CV] END classifier__max_depth=15, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=300; total time=  38.9s\n",
      "[CV] END classifier__max_depth=15, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=150; total time=  18.3s\n",
      "[CV] END classifier__max_depth=15, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=300; total time=  39.1s\n",
      "[CV] END classifier__max_depth=15, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100; total time=  11.8s\n",
      "[CV] END classifier__max_depth=15, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100; total time=  11.8s\n",
      "[CV] END classifier__max_depth=15, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100; total time=  12.1s\n",
      "[CV] END classifier__max_depth=15, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=150; total time=  17.7s\n",
      "[CV] END classifier__max_depth=15, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300; total time=  35.8s\n",
      "[CV] END classifier__max_depth=15, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=150; total time=  17.7s\n",
      "[CV] END classifier__max_depth=15, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300; total time=  35.6s\n",
      "[CV] END classifier__max_depth=15, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=150; total time=  18.1s\n",
      "[CV] END classifier__max_depth=15, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300; total time=  36.1s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=100; total time=  21.8s\n",
      "[CV] END classifier__max_depth=15, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=300; total time=  35.4s\n",
      "[CV] END classifier__max_depth=15, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=300; total time=  35.4s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=100; total time=  22.0s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=100; total time=  22.0s\n",
      "[CV] END classifier__max_depth=15, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=300; total time=  35.7s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=150; total time=  32.3s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=150; total time=  32.9s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=100; total time=  19.6s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=100; total time=  19.3s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=150; total time=  33.0s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=100; total time=  19.7s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=150; total time=  29.0s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=150; total time=  28.9s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=150; total time=  29.4s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=300; total time= 1.1min\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=300; total time= 1.1min\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=1, classifier__min_samples_split=2, classifier__n_estimators=300; total time= 1.1min\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100; total time=  17.4s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100; total time=  17.1s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=100; total time=  17.2s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=300; total time=  58.3s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=300; total time=  57.7s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=150; total time=  25.5s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=150; total time=  25.2s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=150; total time=  25.6s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=300; total time=  58.4s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100; total time=  16.6s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100; total time=  16.7s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100; total time=  16.8s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=150; total time=  24.9s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300; total time=  50.8s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=150; total time=  24.7s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=150; total time=  24.6s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300; total time=  50.0s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=2, classifier__min_samples_split=2, classifier__n_estimators=300; total time=  47.6s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=300; total time=  39.5s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=300; total time=  38.9s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=300; total time=  35.3s\n",
      "Mejor Random Forest: {'classifier__max_depth': 20, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 300}\n",
      "Optimizando XGBoost...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/holamauricios/anaconda3/envs/eda/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [22:08:48] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1733179604375/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/holamauricios/anaconda3/envs/eda/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [22:08:48] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1733179604375/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/holamauricios/anaconda3/envs/eda/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [22:08:48] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1733179604375/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/holamauricios/anaconda3/envs/eda/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [22:08:48] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1733179604375/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/holamauricios/anaconda3/envs/eda/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [22:08:48] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1733179604375/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/holamauricios/anaconda3/envs/eda/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [22:08:48] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1733179604375/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/holamauricios/anaconda3/envs/eda/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [22:08:48] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1733179604375/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/holamauricios/anaconda3/envs/eda/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [22:08:48] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1733179604375/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END classifier__learning_rate=0.01, classifier__max_depth=3, classifier__n_estimators=100; total time=   0.6s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__max_depth=3, classifier__n_estimators=100; total time=   0.6s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__max_depth=3, classifier__n_estimators=100; total time=   0.6s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__max_depth=3, classifier__n_estimators=150; total time=   0.7s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__max_depth=3, classifier__n_estimators=150; total time=   0.7s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__max_depth=3, classifier__n_estimators=150; total time=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/holamauricios/anaconda3/envs/eda/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [22:08:48] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1733179604375/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/holamauricios/anaconda3/envs/eda/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [22:08:48] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1733179604375/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/holamauricios/anaconda3/envs/eda/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [22:08:48] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1733179604375/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/holamauricios/anaconda3/envs/eda/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [22:08:49] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1733179604375/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/holamauricios/anaconda3/envs/eda/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [22:08:49] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1733179604375/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END classifier__learning_rate=0.01, classifier__max_depth=5, classifier__n_estimators=100; total time=   0.7s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__max_depth=5, classifier__n_estimators=100; total time=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/holamauricios/anaconda3/envs/eda/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [22:08:49] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1733179604375/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/holamauricios/anaconda3/envs/eda/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [22:08:49] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1733179604375/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/holamauricios/anaconda3/envs/eda/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [22:08:49] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1733179604375/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=3, classifier__n_estimators=100; total time=   0.5s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=3, classifier__n_estimators=100; total time=   0.6s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__max_depth=5, classifier__n_estimators=100; total time=   0.7s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=3, classifier__n_estimators=100; total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/holamauricios/anaconda3/envs/eda/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [22:08:49] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1733179604375/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/holamauricios/anaconda3/envs/eda/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [22:08:49] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1733179604375/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/holamauricios/anaconda3/envs/eda/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [22:08:49] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1733179604375/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/holamauricios/anaconda3/envs/eda/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [22:08:49] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1733179604375/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=3, classifier__n_estimators=150; total time=   0.7s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__max_depth=5, classifier__n_estimators=150; total time=   1.0s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__max_depth=5, classifier__n_estimators=150; total time=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/holamauricios/anaconda3/envs/eda/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [22:08:49] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1733179604375/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/holamauricios/anaconda3/envs/eda/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [22:08:50] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1733179604375/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/holamauricios/anaconda3/envs/eda/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [22:08:50] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1733179604375/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END classifier__learning_rate=0.01, classifier__max_depth=5, classifier__n_estimators=150; total time=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/holamauricios/anaconda3/envs/eda/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [22:08:50] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1733179604375/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=3, classifier__n_estimators=150; total time=   0.7s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=3, classifier__n_estimators=150; total time=   0.7s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=5, classifier__n_estimators=100; total time=   0.7s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=5, classifier__n_estimators=100; total time=   0.7s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=5, classifier__n_estimators=100; total time=   0.6s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=5, classifier__n_estimators=150; total time=   0.9s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=5, classifier__n_estimators=150; total time=   0.9s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=5, classifier__n_estimators=150; total time=   0.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/holamauricios/anaconda3/envs/eda/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [22:08:51] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1733179604375/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor XGBoost: {'classifier__learning_rate': 0.1, 'classifier__max_depth': 5, 'classifier__n_estimators': 150}\n",
      "Entrenando modelo Stacking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/holamauricios/anaconda3/envs/eda/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [22:08:51] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1733179604375/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[22:09:49] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1733179604375/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "[22:09:49] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1733179604375/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "[22:09:49] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1733179604375/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "[22:09:49] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1733179604375/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "[22:09:49] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1733179604375/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.84      0.86     11139\n",
      "           1       0.85      0.88      0.87     11138\n",
      "\n",
      "    accuracy                           0.86     22277\n",
      "   macro avg       0.86      0.86      0.86     22277\n",
      "weighted avg       0.86      0.86      0.86     22277\n",
      "\n",
      "Guardando modelo en: ../models/stacking_optimized_0.pkl\n",
      "{'0': {'f1-score': 0.8609150087100027,\n",
      "       'precision': 0.8796252927400469,\n",
      "       'recall': 0.8429841098841907,\n",
      "       'support': 11139.0},\n",
      " '1': {'f1-score': 0.8665787159190853,\n",
      "       'precision': 0.8492501292880538,\n",
      "       'recall': 0.8846291973424313,\n",
      "       'support': 11138.0},\n",
      " 'accuracy': 0.8638057189029044,\n",
      " 'macro avg': {'f1-score': 0.863746862314544,\n",
      "               'precision': 0.8644377110140503,\n",
      "               'recall': 0.863806653613311,\n",
      "               'support': 22277.0},\n",
      " 'weighted avg': {'f1-score': 0.8637467351944828,\n",
      "                  'precision': 0.8644383927746881,\n",
      "                  'recall': 0.8638057189029044,\n",
      "                  'support': 22277.0}}\n"
     ]
    }
   ],
   "source": [
    "#model_path = '../models/stacking_optimized_0.pkl'\n",
    "## el param grid está dentro de la función\n",
    "#results = model_methods.training_stacking(df=df_with_outliers,\n",
    "#                                          target_col= 'income',\n",
    "#                                          output_model_path= model_path)\n",
    "\n",
    "#pprint.pprint(results['classification_report'])\n",
    "\n",
    "#---- RESULTADOS: ----\n",
    "#'f1-score': 0.8609150087100027,'precision': 0.8796252927400469,\n",
    "#'accuracy': 0.8638057189029044,\n",
    "\n",
    "#Classification Report:\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.88      0.84      0.86     11139\n",
    "#            1       0.85      0.88      0.87     11138\n",
    "\n",
    "#     accuracy                           0.86     22277\n",
    "#    macro avg       0.86      0.86      0.86     22277\n",
    "# weighted avg       0.86      0.86      0.86     22277"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo parece ser lo suficientemente bueno como para continuar con el despliegue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predition: yes\n",
      "probabilities\n",
      "- no: 5.70%\n",
      "- yes: 94.30%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('yes', 0.057, 'yes')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cargamos el pipeline completo\n",
    "pipeline = joblib.load('../models/stacking_optimized_0.pkl')\n",
    "\n",
    "# nuevos datos para predicción\n",
    "new_data = pd.DataFrame({'age'   :[26],\n",
    "                         'hours_per_week':[56],\n",
    "                         'marital_status':['married-civ-spouse'],\n",
    "                         'relationship'  :['husband'],\n",
    "                         'education'     :['bachelors'],\n",
    "                         'occupation'    :['exec-managerial'],\n",
    "                         'capital_net'   : [10000],})\n",
    "\n",
    "def translate_pred (pipeline, new_data: pd.DataFrame, return_prob: bool= False) -> str|tuple:\n",
    "    pred = pipeline.predict(new_data)\n",
    "    prob = pipeline.predict_proba(new_data)\n",
    "    \n",
    "    #traducción de la predicción a str\n",
    "    pred_class = 'yes' if int(pred[0]) == 1 else 'no'\n",
    "    \n",
    "    prob_no  = round(prob[0][0] , 3) # probabilidad de que sea 'no'\n",
    "    prob_yes = round(prob[0][1] , 3) # probabilidad de que sea 'yes'\n",
    "    \n",
    "    print(f'predition: {pred_class}\\nprobabilities\\n- no: {prob_no:.2%}\\n- yes: {prob_yes:.2%}')\n",
    "    return pred_class, prob_no, prob_yes if return_prob else pred_class     \n",
    "    \n",
    "    \n",
    "translate_pred(pipeline, new_data, return_prob= False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b style=\"font-size: 1.5em;\">🎉 Entrenamiento terminado</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
