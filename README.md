# ğŸ’¸ PredicciÃ³n de Ingresos Anuales a partir de Datos demogrÃ¡ficos ğŸ’¸

## ğŸ“ DescripciÃ³n del proyecto
Este proyecto se centra en la predicciÃ³n de ingresos anuales usando un dataset del *Census Bureau* de los EEUU. Contiene informaciÃ³n demogrÃ¡fica y econÃ³mica de mÃ¡s de $45,000$ sujetos. El objetivo principal es predecir si una persona tiene un ingreso mayor a $USD $50$ k al aÃ±o basado en sus caracterÃ­sticas personales y laborales.
* *(bitÃ¡cora al final del README)*

incluye las siguientes variables:
(*"$n$ tipos distintos"* $\rightarrow$ equivale al nÃºmero de elementos/categorÃ­as diferentes que hay en cada columna **categÃ³rica**): 
* **(1) age**: edad del sujeto
* **(2) workclass**: Tipo de trabajo / sector al que pertenece el individuo *(gobierno, privado, sin empleo, etc.)*. $9$ tipos distintos
* **(3) fnlwgt**: Peso final del individup en la encuesta (descripciÃ³n de esta variable mÃ¡s abajo) 
* **(4) education_num**: Nivel educativo del individuo *(en formato numÃ©rico)* correspondiente a los aÃ±os de educaciÃ³n completados
* **(5) marital_status**: Estado civil del individuo *(soltero, casado)*
* **(6) occupation**: A quÃ© se dedica *(ejecutivo, obrero, empleado de gobierno, etc)*. $15$ tipos distintos
* **(7) relationship**: Rol **familiar** que el individuo asume dentro del hogar *(jefe de hogar, esposo/a)*. $6$ tipos distintos
* **(8) ethnicity**: Etnia del individuo *(Blanco, negro, asiÃ¡tico)*. $5$ tipos distintos
* **(9) genre**: gÃ©nero del individuo. $2$ tipos distintos
* **(10) capital_gain**: Ganancias de vapital adicionales, fuera del salario o ingresos laborales
* **(11) capital_loss**: PÃ©rdidas de capital adicionales
* **(12) hours_per_week**: Horas trabajadas por semana
* **(14) native_country**: PaÃ­s de origen del indiviuo
* **(15) income**: Variable objetivo.
    * no $=$ salario USD$<= 50k$ anuales **(individuo gana 50k anuales o menos)**
    * yes  $=$ salario USD$> 50k$ anuales **(individuo gana mÃ¡s de 50k anuales)**

## ğŸ› ï¸ Deployment del modelo
El modelo de clasificaciÃ³n entrenado durante este proyecto ha sido desplegado en **streamlit.io**.
* Puedes ingresar a: "[Income Prediction APP](https://mauricios11-random-forest-deployment.streamlit.app)" para probarlo. *(estÃ¡ en inglÃ©s porque ando muy bilingÃ¼e ğŸ¤­ :D)*
* El repositorio del deployment estÃ¡ en el siguiente enlace: "[Random Forest Deployment](https://github.com/mauricios11/random_forest_deployment)"

### ğŸ“š AclaraciÃ³n de entornos implementados
Durante el anÃ¡lisis exploratorio hemos trabajado con dos entornos distintos. La razÃ³n de esto es porque la librerÃ­a encargada de hacer el balanceo de datos necesita un versiÃ³n especÃ­fica de *sklearn*, una versiÃ³n anterior cuya compatibilidad con *seaborn* y otras librerÃ­as da algunos problemas.
* teÃ³ricamente, la diferencia principal entre ambos entornos es que el que estÃ¡ hecho para datos desbalanceados usa `scikit-learn==1.2.2` + `imblearn`
* en adiciÃ³n, se hizo un downgrade en el entorno *eda* a `scikit-learn==1.5.2` por un festival de warnings que salÃ­an al usar `xgboost`

### â„¹ï¸ instalaciÃ³n del proyecto:
âœ‚ï¸ En un nuevo directorio para el proyecto, ejecuta el siguiente comando:
```


```
âœ‚ï¸ entorno 1: (eda):
```
#anaconda
conda env create -f eda_environment.yml

#pip
pip install --upgrade pip
pip install -r eda_requirements.txt

```
âœ‚ï¸ entorno 2: (imbalanced) 
```
#anaconda (instalar imblearn con pip)
conda env create -f imbalanced_environment.yml
pip install imblearn

#pip
pip install --upgrade pip
pip install -r imbalanced_requirements.txt
```
Si tienen algÃºn problema con la instalaciÃ³n de librerÃ­as $\rightarrow$ just hit me up :D *(recomiendo usar anaconda)*

### ğŸ“ Estructura del proyecto
```
â”œâ”€â”€ data
â”‚Â Â  â”œâ”€â”€ clean
â”‚Â Â  â””â”€â”€ raw
â”‚Â Â      â”œâ”€â”€ adult.csv
â”‚Â Â      â”œâ”€â”€ adult_complete.csv
â”‚Â Â      â”œâ”€â”€ adult_test.test
â”‚Â Â      â””â”€â”€ adult_validation.csv
â”œâ”€â”€ description.txt
â”œâ”€â”€ modules                           # MÃ³dulos auxiliares (listas, diccionarios, mÃ©todos)
â”‚Â Â  â”œâ”€â”€ list_and_dicts.py             # listas y diccionarios
â”‚Â Â  â”œâ”€â”€ module_discarted_methods.py   # registro de procesos descartados durante el anÃ¡lisis (bajo desempeÃ±o)
â”‚Â Â  â”œâ”€â”€ module_imbalanced.py          # mÃ©todos implementados durande el procedo de balanceo
â”‚Â Â  â”œâ”€â”€ module_imputation.py          # para el proceso de imputaciÃ³n
â”‚Â Â  â”œâ”€â”€ module_modeling.py            # entrenamiento de varios modelos
â”‚Â Â  â””â”€â”€ import_modules.py             # Archivo para manejar todas las importaciones al nb
â”œâ”€â”€ models                            # Modelos entrenados (leer txt)
â”œâ”€â”€ notebooks
â”‚Â Â  â”œâ”€â”€ 00_initial_EDA.ipynb          # EDA inicial
â”‚Â Â  â”œâ”€â”€ 01_EDA_missing.ipynb          # EDA e imputaciÃ³n para valores nulos
â”‚Â Â  â”œâ”€â”€ 02_imbalanced.ipynb           # balanceo de target con dos diferentes mÃ©todos
â”‚Â Â  â”œâ”€â”€ 03_model_training.ipynb       # entrenamiento del modelo final (diferentes mÃ©todos)
â”‚Â Â  â”œâ”€â”€ 03_training_input.txt         # muestra de uno de los inputs del entrenamiento
â”‚Â Â  â””â”€â”€ import_modules.py             # Archivo para manejar todas las importaciones al nb
â”œâ”€â”€ scripts
â”‚Â Â  â”œâ”€â”€ load_data.py                  # Funciones para cargar los datos
â”‚Â Â  â”œâ”€â”€ utils.py                      # Funciones generales
â”‚Â Â  â”œâ”€â”€ utils_categorical_plots.py    # GrÃ¡ficos para variables categÃ³ricas
â”‚Â Â  â”œâ”€â”€ utils_classif_models_plots.py # GrÃ¡ficos relacionados con modelos de clasificaciÃ³n
â”‚Â Â  â”œâ”€â”€ utils_initial_exploration.py  # ExploraciÃ³n inicial
â”‚Â Â  â”œâ”€â”€ utils_missing_extension.py    # Extensiones para manejo de nulos en pandas
â”‚Â Â  â”œâ”€â”€ utils_missing_extension_plots.py # GrÃ¡ficos para manejo de nulos
â”‚Â Â  â””â”€â”€ utils_probability_funcs.py    # Funciones para cÃ¡lculos probabilÃ­sticos
```

### ğŸ“° BitÃ¡cora de procesos

se comenzÃ³ con un EDA, analizando el comportamiento de las variables en general, sin encontrar muchas columnas relaciÃ³n lineal significativa (posteriormente se hizo un anÃ¡lisis para encontrar relaciones no lineales)

se detectaron valores nulos implÃ­citos como 'unknnown', en adiciÃ³n se ancontrÃ³ otro tipo de columnas con prefijo "_other" las cuales se decidiÃ³ no tocar dado que se trataba de registros reales pero poco comunes (por eso no figuraban como categorÃ­a con un nombre especÃ­fico)

imputaciÃ³n por la moda e imputaciÃ³n por medio de un DecisionTreeClassifier (el segundo con mejor desempeÃ±o)
balanceo por SMOTE (oversampling a minoritarias) y se encontrÃ³ el mejor punto para tener la cantidad de muestras tal que diera el mejor f1-score. despuÃ©s se hizo un  balanceo por subsets (oversampling a minoritarias + undersampling con RandomForestClassifier)
el smote termino siendo ligeramente mejor

continuamos con la selecciÃ³n de las columnas mÃ¡s importantes mediante feature importances y mutual information

comparamos el desempeÃ±o con un PCA, pero resultÃ³ ser menor (tiene sentido porque no hay muchas columnas, y las que hay en su mayorÃ­a son categÃ³ritas)
    <p>Durante el anÃ¡lisis exploratorio en pasos anteriores encontramos que una de las variables mÃ¡s importantes en el anÃ¡lisis inicial fue <i>fnlwgt</i>. Sin embargo, al planear el despliegue del modelo surgiÃ³ una preocupaciÃ³n:</p> 
    <ul>
        <li>El usuario no tendrÃ­a acceso a esta cifra <i>(fnlwgt)</i> sin los datos y procesos especÃ­ficos del dataset original. Por esta razÃ³n, aceptamos el reto de crear un modelo capaz de predecir este valor, dada su importancia en el desempeÃ±o del modelo principal.</li>
    </ul>
    <p><b>Estrategia</b>:</p>
    <ul>
        <li>Se exploraron varios algoritmos: <i>Random Forest Regressor, XGBoost, LightGBM, SVR</i> junto con una estrategia de Modelos de ensamble<i>(stacking)</i> combinando varios de los anteriores.</li>
    </ul>
    <p>Todos estos mÃ©todos fueron optimizados mediante la bÃºsqueda de hiperparÃ¡metros (GridSearchCV) y ajustes adicionales como:</p>
    <ul>
        <li>PCA para reducciÃ³n de dimensionalidad.
        <li>IncorporaciÃ³n de variables derivadas como <code>capital_net</code> <i>(basado en capital_gain y capital_loss).</i></li>
        <li>Variantes con distintos conjuntos de columnas consideradas importantes.</li>
    </ul>
    <p>A pesar de las mÃºltiples iteraciones con distintos mÃ©todos, los resultados en todos los casos tuvieron un desmempeÃ±o poco aceptable</p>
    <ul>
        <li>La mÃ©tricas: <b>RÂ², MAE y RMSE</b> (haciendo incapiÃ© en la primera) reflejaron un modelo incapaz de generalizar la complejidad de <i>fnlwgt</i></li>
    </ul>
<b>reflexiÃ³n</b>: A pesar de haber tenido indicios de que esta variable podrÃ­a ser problemÃ¡tica desde el principio del anÃ¡lisis exploratorio, ignoramos tal evidencia y continuamos con el anÃ¡lisis, aceptando el reto de enfrentarse a predecir esta variable.

* Esta es una prueba sobre la importancia sobre evitar ignorar indicios que los datos nos dan al princpio del EDA, y que no todos los valores importantes en un anÃ¡lisis inicial son predicibles en un contexto prÃ¡ctico. fnlwgt, aunque relevante para predecir income, no fue posible de modelar con la precisiÃ³n deseada <>(incluso tras extensos ajustes)
* A veces es mÃ¡s eficiente reevaluar y rediseÃ±ar estrategias en lugar de insistir en una soluciÃ³n que no es factible en el contexto prÃ¡ctico.

 <b style="font-size: 1.5em;">ğŸ’­ Nueva estrategia</b>
    <p>Se descarta <code>fnlwgt</code>como variable en el modelo final que busca predecir <code>income</code>. Para compensar su ausencia, se agregarÃ¡n mÃ¡s columnas presentes en las <b>feature importances</b> | <b>mutual information</b></p>
    <br>
    <p>Para contrarrestar la pÃ©rdida de <code>fnlwgt</code> evaluaremos diferentes instancias del df con las columnas mÃ¡s importantes:</p>
    <ul>
        <li> <b>df_no_capital</b>: sin agregar mÃ¡s columnas (total de 7)</li>
        <li> <b>df_capital_gain</b>: agregando otra feature importance con menos relevancia: <code>capital_gain</code> (total de 8)
        <li> <b>df_capital_net</b>: haciendo una columna nueva: <code>capital_net</code> <i>(direfencia entre capital_gain y capital_loss)</i> (total de 8)</li>
    </ul>
</div>

finalmente entrenamos un RandomForest classifier con un gridsearch para encontrar los mejores parÃ¡metros, aunque se descartÃ³ por tener un bajo desempeÃ±o, el modelo que ha funcionado es un stacking de random forest classifier + xgboost classifier (cada uno con un grid search) administrando el proceso por un pipeline

despuÃ©s de encontrar el mejor modelo, exportamos el cÃ³digo necesario para desplegar el modelo a streamlit. 
* durante el despliegue nos encontramos con que el modelo en cuestiÃ³n era demasiado pesado, gracias a Git LFS. aunuqe otra alternativa hubiera sido comprimir el modelo.



